<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>PocketSphinx.js</title>
  </head>
  <body>
    <h2>PocketSphinx.js live demo</h2>
    <ul>
      <li>This demo works on recent versions of Chrome and Firefox with the Web Audio API, make sure it works for you and actually records audio.</li>
      <li>Press "Start" and speak</li>
    </ul>
    <select id="grammars"></select>
    <button id="startBtn">Start</button>
    <button id="stopBtn">Stop</button>
    <span id="recording-indicator" style="border-radius: 10px; -moz-border-radius: 10px; -webkit-border-radius: 10px; width: 20px; height: 20px; background: red;"></span>
    <h2>Recognition Output</h2>
    <div id="output" style="height:150px;overflow:auto;" >
    </div>
    <h2>Status</h2>
    <div id="current-status">Loading page</div>

    <script type="module">
      import MicrophoneWorker from './avatars/microphone-worker.js';

      const _makePromise = () => {
        let resolve, reject;
        const promise = new Promise((res, rej) => {
          resolve = res;
          reject = rej;
        });
        promise.accept = resolve;
        promise.reject = reject;
        return promise;
      };

      class CallbackManager {
        constructor() {
          this.currentId = 0;
          this.callbackPool = {};
        }
        add(clb) {
          var id = this.currentId;
          this.callbackPool[id] = clb;
          this.currentId++;
          return id;
        }
        get(id) {
          if (this.callbackPool.hasOwnProperty(id)) {
            var clb = this.callbackPool[id];
            delete this.callbackPool[id];
            return clb;
          }
          return null;
        }
      };
      class AudioRecorder extends EventTarget {
        constructor(audioContext, consumer) {
          super();

          this.audioContext = audioContext;
          this.consumer = consumer;
        }
        start() {
          const u = '/sounds/pissbaby.mp3';
          const audio = new Audio(u);
          audio.controls = true;
          audio.loop = true;
          audio.addEventListener('canplaythrough', async e => {
            const options = {
              muted: false,
              // emitVolume: true,
              emitBuffer: true,
              audioContext: this.audioContext,
              // microphoneWorkletUrl: 'microphone-worklet.js',
              microphoneWorkletUrl: '/avatars/microphone-worklet.js',
            }
            const microphoneWorker = new MicrophoneWorker(audio, options);
            microphoneWorker.addEventListener('volume', e => {
              console.log('got volume', e);
            });
            const record = (() => {
              const config = {};
              let recBuffers = [];
              var inputBufferLength = config.inputBufferLength || 4096;
              var outputBufferLength = config.outputBufferLength || 4000;
              const inSampleRate = audioContext.sampleRate;
              const outputSampleRate = config.outputSampleRate || 16000;
              
              return inputBuffer => {
                var isSilent = true;
                for (var i = 0 ; i < inputBuffer[0].length ; i++) {
                  recBuffers.push((inputBuffer[0][i] + inputBuffer[1][i]) * 16383.0);
                }
                while (recBuffers.length * outputSampleRate / inSampleRate > outputBufferLength) {
                  var result = new Int16Array(outputBufferLength);
                  var bin = 0,
                  num = 0,
                  indexIn = 0,
                  indexOut = 0;
                  while (indexIn < outputBufferLength) {
                    bin = 0;
                    num = 0;
                    while(indexOut < Math.min(recBuffers.length, (indexIn + 1) * inSampleRate / outputSampleRate)) {
                      bin += recBuffers[indexOut];
                      num += 1;
                      indexOut++;
                    }
                    result[indexIn] = bin / num;
                    if(isSilent && (result[indexIn] != 0)) isSilent = false;
                    indexIn++;
                  }
                  var output = {};
                  output.command = 'newBuffer';
                  output.data = result;
                  if (isSilent) output.error = "silent";
                  
                  // this.consumer.postMessage({ command: 'process', data: result });
                  this.consumer.send(result);
                  
                  recBuffers = recBuffers.slice(indexOut);
                }
              };
            })();
            microphoneWorker.addEventListener('buffer', e => {
              const {method, data} = e.data;
              
              const inputBuffer = [
                data,
                data,
              ];
              record(inputBuffer);
            });
            this.microphoneWorker = microphoneWorker;

            await microphoneWorker.waitForLoad();

            audio.play();
            this.audioContext.resume();
          }, {once: true});
          audio.addEventListener('error', e => {
            console.log('load error', e);
          });
        }
        stop() {
          // this.consumer.postMessage({ command: 'stop' });
          this.microphoneWorker.close();
        }
        cancel() {
          this.stop();
        }
      }
      class AudioRecognizer {
        constructor() {
          this.loadPromise = _makePromise();
          
          this.callbackManager = new CallbackManager();

          const workerURL = '/pocketsphinx.js/webapp/js/recognizer.js';
          this.worker = new Worker(workerURL);
          this.worker.onmessage = event => {
            // onReady(recognizer);

            // console.log('ready event', event);
            this.loadPromise.accept();

            this.worker.onmessage = e => {
              // This is the case when we have a callback id to be called
              if (e.data.hasOwnProperty('id')) {
                var clb = this.callbackManager.get(e.data['id']);
                var data = {};
                if ( e.data.hasOwnProperty('data')) data = e.data.data;
                if(clb) clb(data);
              }
              // This is a case when the recognizer has a new hypothesis
              if (e.data.hasOwnProperty('hyp')) {
                var newHyp = e.data.hyp;
                if (e.data.hasOwnProperty('final') &&  e.data.final) newHyp = "Final: " + newHyp;
                updateHyp(newHyp);
              }
              // This is the case when we have an error
              if (e.data.hasOwnProperty('status') && (e.data.status == "error")) {
                updateStatus("Error in " + e.data.command + " with code " + e.data.code);
              }
            };

            (async () => {
              // console.log('got 1');
              await this.postRecognizerJob({
                command: 'lazyLoad',
                data: {
                  // pocketsphinx/model/en-us/en-us-phone.lm.bin
                  // folders: [["/", "zh_broadcastnews_ptm256_8000"]],
                  folders: [],
                  files: [
                    ["/", "en-us-phone.lm.bin", "../../../model/en-us/en-us-phone.lm.bin"],
                    /* ["/zh_broadcastnews_ptm256_8000", "means", "../zh_broadcastnews_ptm256_8000/means"],
                    ["/zh_broadcastnews_ptm256_8000", "variances", "../zh_broadcastnews_ptm256_8000/variances"],
                    ["/zh_broadcastnews_ptm256_8000", "transition_matrices", "../zh_broadcastnews_ptm256_8000/transition_matrices"],
                    ["/zh_broadcastnews_ptm256_8000", "sendump", "../zh_broadcastnews_ptm256_8000/sendump"],
                    ["/zh_broadcastnews_ptm256_8000", "mdef", "../zh_broadcastnews_ptm256_8000/mdef"],
                    ["/zh_broadcastnews_ptm256_8000", "feat.params", "../zh_broadcastnews_ptm256_8000/feat.params"],
                    ["/zh_broadcastnews_ptm256_8000", "mixture_weights", "../zh_broadcastnews_ptm256_8000/mixture_weights"],
                    ["/zh_broadcastnews_ptm256_8000", "noisedict", "../zh_broadcastnews_ptm256_8000/noisedict"] */
                    // ["/", "kws.txt", "../kws.txt"],
                    // ["/", "kws.dict", "../kws.dict"],
                  ],
                },
              });
              // console.log('got 2');
              await this.postRecognizerJob(
                {
                  command: 'initialize',
                  data: [/*["-kws", "kws.txt"], ["-dict","kws.dict"], */ ['-allphone', 'en-us-phone.lm.bin']],
                }
              );

              this.worker.postMessage({ command: 'start', data: '' });
              
              recognizerReady();

              // console.log('got 3');
            })();
          };
          // As arguments, you can pass non-default path to pocketsphinx.js and pocketsphinx.wasm:
          // recognizer.postMessage({'pocketsphinx.wasm': '/path/to/pocketsphinx.wasm', 'pocketsphinx.js': '/path/to/pocketsphinx.js'});
          this.worker.postMessage({
            'pocketsphinx.js': 'pocketsphinx.js',
            'pocketsphinx.wasm': 'pocketsphinx.wasm',
          });
        }
        postRecognizerJob(message) {
          return new Promise((accept, reject) => {
            var msg = message || {};
            msg.callbackId = this.callbackManager.add(accept);
            this.worker.postMessage(msg);
          });
        }
        waitForLoad() {
          return this.loadPromise;
        }
        send(result) {
          this.worker.postMessage({ command: 'process', data: result });
        }
        destroy() {
          this.worker.postMessage({ command: 'stop' });
        }
      }

      let isRecognizerReady = false;

      // These will be initialized later
      var audioRecognizer, audioRecorder, audioContext, outputContainer;
      // Only when both recorder and recognizer do we have a ready application
      var isRecorderReady = isRecognizerReady = false;

      // To display the hypothesis sent by the recognizer
      function updateHyp(hyp) {
        if (outputContainer) outputContainer.innerHTML = hyp;
      };

      // This updates the UI when the app might get ready
      // Only when both recorder and recognizer are ready do we enable the buttons
      function updateUI() {
        if (isRecorderReady && isRecognizerReady) startBtn.disabled = stopBtn.disabled = false;
      };

      // This is just a logging window where we display the status
      function updateStatus(newStatus) {
        document.getElementById('current-status').innerHTML += "<br/>" + newStatus;
      };

      // A not-so-great recording indicator
      function displayRecording(display) {
        if (display) document.getElementById('recording-indicator').innerHTML = "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;";
        else document.getElementById('recording-indicator').innerHTML = "";
      };

      // This starts recording. We first need to get the id of the grammar to use
      var startRecording = function() {
        audioRecorder.start();
        displayRecording(true);
      };

      // Stops recording
      var stopRecording = function() {
        audioRecorder && audioRecorder.stop();
        displayRecording(false);
      };

      // Called once the recognizer is ready
      // We then add the grammars to the input select tag and update the UI
      var recognizerReady = function() {
           // updateGrammars();
           isRecognizerReady = true;
           updateUI();
           updateStatus("Recognizer ready");
      };

      // When the page is loaded, we spawn a new recognizer worker and call getUserMedia to
      // request access to the microphone
      window.onload = function() {
        outputContainer = document.getElementById("output");
        updateStatus("Initializing web audio and speech recognizer, waiting for approval to access the microphone");

        // The following is to initialize Web Audio
        try {
          window.AudioContext = window.AudioContext || window.webkitAudioContext;
          window.URL = window.URL || window.webkitURL;
          audioContext = new AudioContext();
          window.audioContext = audioContext;
        } catch (e) {
          updateStatus("Error initializing Web Audio browser");
        }

        audioRecognizer = new AudioRecognizer(); 
        audioRecorder = new AudioRecorder(audioContext, audioRecognizer);
        isRecorderReady = true;
        updateUI();
        updateStatus("Audio recorder ready");

      // Wiring JavaScript to the UI
      var startBtn = document.getElementById('startBtn');
      var stopBtn = document.getElementById('stopBtn');
      startBtn.disabled = true;
      stopBtn.disabled = true;
      startBtn.onclick = startRecording;
      stopBtn.onclick = stopRecording;
    };
    </script>
  </body>
</html>
